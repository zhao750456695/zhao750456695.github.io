<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>tensorflow2 | FinGeek</title>
  <meta name="author" content="JieZhao">
  
  <meta name="description" content="TensorFlow学习笔记">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="tensorflow2"/>
  <meta property="og:site_name" content="FinGeek"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">FinGeek</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class=""></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class=""></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class=""></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class=""></i>About
			</a>
		  </li>
		  
		  <li>
			<a href="/atom.xml" title="Subscribe me.">
			  <i class=""></i>RSS
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> tensorflow2</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> TensorFlow学习笔记
		 </div> <!-- alert -->
	  		

	  <h1 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h1><h2 id="3-1-张量、计算图、会话"><a href="#3-1-张量、计算图、会话" class="headerlink" title="3.1 张量、计算图、会话"></a>3.1 张量、计算图、会话</h2><p>基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重(参数)，得到模型。</p>
<p><strong>张量（tensor）：</strong>多维数组（列表） </p>
<p><strong>阶：</strong>张量的维数</p>
<table>
<thead>
<tr>
<th style="text-align:center">维数</th>
<th style="text-align:center">阶</th>
<th style="text-align:center">名字</th>
<th style="text-align:center">例子</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0-D</td>
<td style="text-align:center">0</td>
<td style="text-align:center">标量 scalar</td>
<td style="text-align:center">s=1 2 3</td>
</tr>
<tr>
<td style="text-align:center">1-D</td>
<td style="text-align:center">1</td>
<td style="text-align:center">向量 vector</td>
<td style="text-align:center">v=[1,2,3]</td>
</tr>
<tr>
<td style="text-align:center">2-D</td>
<td style="text-align:center">2</td>
<td style="text-align:center">矩阵 matrix</td>
<td style="text-align:center">m[[1,2,3],[4,5,6],[7,8,9]] 二维数组</td>
</tr>
<tr>
<td style="text-align:center">n-D</td>
<td style="text-align:center">n</td>
<td style="text-align:center">张量 tensor</td>
<td style="text-align:center">t=[[[…      n个    方括号有几个就是几阶的</td>
</tr>
</tbody>
</table>
<p>张量可以表示0阶到n阶数组(列表)</p>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>tf.float32 </p>
<p>tf.int32</p>
<p>加法运算</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([1.0, 2.0]) # 定义张量a constant常数</span><br><span class="line">b = tf.constant([3.0, 4.0]) # 定义张量b </span><br><span class="line">result = a+b</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>结果是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;add:0&quot;, shape=(2,), dtype=float32)</span><br><span class="line">#result是一个名为&apos;add:0&apos;的张量，add是节点名，0表示第0个输出。shape是维度，因为这里是一维数组，只有一个元素2，表示一维数组长度为2，dtype是数据类型。</span><br></pre></td></tr></table></figure>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p><strong>计算图（Graph）:</strong>搭建神经网络的计算过程，只搭建，不运算。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/c3a5a043ly1fq9kmhllikj20dd0d7441.jpg" alt=""></p>
<p>上面是神经元的基本模型，其实就是数学里的乘加运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]])</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tensor(<span class="string">"MatMul:0"</span>, shape=(<span class="number">1</span>, <span class="number">1</span>), dtype=float32)</span><br><span class="line"><span class="comment"># shape中有两个元素，是两维的，一行一列</span></span><br><span class="line"><span class="comment"># 只搭建了一个图</span></span><br></pre></td></tr></table></figure>
<p>想执行图就必须用到会话了</p>
<p><strong>会话（Session）：</strong>执行计算图中的节点运算。</p>
<p>用with</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">	print(sess.run(y))</span><br><span class="line"><span class="comment"># Session首字母大写</span></span><br><span class="line"><span class="comment"># sess.run()运行</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">w = tf.constant([[<span class="number">3.0</span>], [<span class="number">4.0</span>]])</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">print(y)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(y))</span><br></pre></td></tr></table></figure>
<p>结果是</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[ 11.]]</span><br></pre></td></tr></table></figure>
<p>ubuntu下运行出现一些警告信息</p>
<p>可以把它们去掉</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>把tensorflow的提示登记降低，加入下面这句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export TF_CPP_MIN_LOG_LEVEL=2</span><br></pre></td></tr></table></figure>
<p>然后，让配置文件生效</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>
<h2 id="3-2-前向传播"><a href="#3-2-前向传播" class="headerlink" title="3.2 前向传播"></a>3.2 前向传播</h2><p><img src="http://wx3.sinaimg.cn/mw690/c3a5a043ly1fq9laniw1ej20wk0ewqhs.jpg" alt=""></p>
<p>初值随机生成，用<code>tf.Variable()</code>意为生成随机数，V要大写，生成方式写在括号里</p>
<p>上图中随机种子如果去掉每次生成的随机数将不一致</p>
<p>标准差、均值、随机种子可以不写</p>
<p>除了<code>tf.random_normal()</code>，还有很多随机数生成方式，如<code>tf.truncated_normal()</code>如果生成出的随机数超过平均值两个标准差将重新生成，<code>tf.random_uniform()</code>等</p>
<p>除了随机数，还可以生成常量</p>
<table>
<thead>
<tr>
<th style="text-align:center">方式</th>
<th style="text-align:center">描述</th>
<th style="text-align:center">举例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tf.ones</td>
<td style="text-align:center">全1数组</td>
<td style="text-align:center">tf.ones([3,2],int32)生成[[1,1],[1,1][1,1]]</td>
</tr>
<tr>
<td style="text-align:center">tf.fill</td>
<td style="text-align:center">全定值数组</td>
<td style="text-align:center">tf.fill([3,2],6)生成[[6,6],[6,6],[6,6]]</td>
</tr>
<tr>
<td style="text-align:center">tf.constant</td>
<td style="text-align:center">直接给值</td>
<td style="text-align:center">tf.constant([3,2,1])生成[3,2,1]</td>
</tr>
<tr>
<td style="text-align:center">tf.zeros</td>
<td style="text-align:center">全0数组</td>
<td style="text-align:center">tf.zeros([3,2],int32)生成[[0,0],[0,0][0,0]]</td>
</tr>
</tbody>
</table>
<p><strong>神经网络的实现过程：</strong></p>
<p>1、准备数据集，提取特征，作为输入喂给神经网络（Netural Network,NN）</p>
<p>2、搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）</p>
<p>（NN前向传播算法———-&gt;计算输出）</p>
<p>3、大量特征数据喂给NN，迭代优化NN参数</p>
<p>（NN反向传播算法———-&gt;优化参数训练模型）</p>
<p>4、使用训练好的模型预测和分类</p>
<p>因此，神经网络的使用过程可以分两步：1-3的循环迭代是第一步，即训练，4是第二步，是使用过程。</p>
<blockquote>
<p>一旦参数优化完成就可以固定这些参数实现特定应用，很多现实应用中会使用成熟的现有网络结构，喂入新的数据，训练相应模型，判断是否能对喂入的、从未见过的新数据做出正确的响应。再适当更改网络结构、反复迭代，让机器自动训练参数，找出最优结构和参数，固定专有网络。</p>
</blockquote>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p><img src="http://wx3.sinaimg.cn/mw690/c3a5a043ly1fq9mjlalf3j20vs0kytqn.jpg" alt=""></p>
<p><img src="http://wx2.sinaimg.cn/mw690/c3a5a043ly1fq9mrcas6tj20wm0mzts9.jpg" alt=""></p>
<p>第一层的w前面有两个两个节点，后面有三个节点，因此是个两行三列的矩阵</p>
<p>a是第一个计算层，一般说神经网络共有几层、当前是第几层都是指的计算层，输入不是计算层，因此a是第一层网络</p>
<p>第二层的参数w前面是是3个节点、后面是1个节点，因此是三行一列的矩阵</p>
<p> <img src="http://wx4.sinaimg.cn/mw690/c3a5a043ly1fq9n2sa1gzj20tc0lph5o.jpg" alt=""></p>
<p>tf.placeholder()中shape第一个元素表示几组数据，第二个元素表示每组数据有几个特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两层简单的神经网络(全连接)</span></span><br><span class="line">x = tf.constant([[<span class="number">0.7</span>, <span class="number">0.5</span>]])</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 =  tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer() <span class="comment"># 初始化</span></span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(<span class="string">'y in tf3.py is :\n'</span>, sess.run(y))</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y <span class="keyword">in</span> tf3.py <span class="keyword">is</span> :</span><br><span class="line"> [[ <span class="number">3.0904665</span>]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 两层简单的神经网络</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder实现输入定义 （see.run中喂一组数据）</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(<span class="string">'y in tf3_4.py is:\n'</span>, sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>]]&#125;))</span><br></pre></td></tr></table></figure>
<p>结果是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y in tf3_4.py is:</span><br><span class="line"> [[ 3.0904665]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder实现输入定义 （see.run中喂多组数据）</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="keyword">None</span>, <span class="number">2</span>)) <span class="comment"># 不知道喂多少组，第一个元素是None</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播过程</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    print(<span class="string">'y in tf3_4.py is:\n'</span>, sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.3</span> , <span class="number">0.4</span>], [<span class="number">0.4</span>, <span class="number">0.5</span>]]&#125;))</span><br><span class="line">    print(<span class="string">'w1:\n'</span>, sess.run(w1))</span><br><span class="line">    print(<span class="string">'w2:\n'</span>, sess.run(w2))</span><br></pre></td></tr></table></figure>
<p>结果是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">y in tf3_4.py is:</span><br><span class="line"> [[ 3.0904665 ]</span><br><span class="line"> [ 1.2236414 ]</span><br><span class="line"> [ 1.72707319]</span><br><span class="line"> [ 2.23050475]]</span><br><span class="line">w1:</span><br><span class="line"> [[-0.81131822  1.48459876  0.06532937]</span><br><span class="line"> [-2.4427042   0.0992484   0.59122431]]</span><br><span class="line">w2:</span><br><span class="line"> [[-0.81131822]</span><br><span class="line"> [ 1.48459876]</span><br><span class="line"> [ 0.06532937]]</span><br></pre></td></tr></table></figure>
<h2 id="3-3-反向传播"><a href="#3-3-反向传播" class="headerlink" title="3.3 反向传播"></a>3.3 反向传播</h2><p><img src="http://wx3.sinaimg.cn/mw690/c3a5a043ly1fq9owelqbjj20vd0ktx1f.jpg" alt=""></p>
<p>搭建神经网络的八股：准备、前传、后传、迭代</p>
<p>0准备  import</p>
<p>​            常量定义</p>
<p>​            生成数据集</p>
<p>1前向传播： 定义输入、参数和输出</p>
<p>​              x=</p>
<p>​              y_=</p>
<p>​              w1=</p>
<p>​               w2=</p>
<p>​                a=</p>
<p>​                 y=</p>
<p>2反向传播：定义损失函数、反向传播方法</p>
<p>​                loss=</p>
<p>​                train_step=</p>
<p>3生成对话，训练STEPS轮</p>
<p>with tf.Session() as sess</p>
<p>​    init_opt = tf.global_variables_initializer()</p>
<p>​    sess_run(init_op)</p>
<p>​    STEP =     3000</p>
<p>​    for i in range(STEPS):</p>
<p>​        start =</p>
<p>​        end = </p>
<p>​        sess.run(train_step, feed_dict:)</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>Prev</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2018/04/12/R语言part1/" type="button" class="btn btn-default ">Next<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">Comments</h2>

    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2018-04-12 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/深度学习/">深度学习<span>1</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/tensorflow-深度学习/">tensorflow 深度学习<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2018 JieZhao
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    
</p>
 </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
